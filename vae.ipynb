{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The Variational Autoencoder will now be implemented and its performance assessed using the MNIST digits and Fashion MNIST datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:10:03.317215Z",
     "iopub.status.busy": "2023-07-23T17:10:03.316455Z",
     "iopub.status.idle": "2023-07-23T17:10:08.502442Z",
     "shell.execute_reply": "2023-07-23T17:10:08.501275Z",
     "shell.execute_reply.started": "2023-07-23T17:10:03.317179Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "ACCELERATOR = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The Torchvision library provides access to the MNIST digits and Fashion MNIST datasets. For further processing, these datasets will be enveloped with transformations such as:\n",
    "- Conversion from PIL image to torch tensor\n",
    "- Standardization\n",
    "\n",
    "The estimated mean for the MNIST dataset is 0.13, and the standard deviation is 0.31. These estimates can be readily verified by computing the mean and std across all pixels in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:10:08.505428Z",
     "iopub.status.busy": "2023-07-23T17:10:08.504594Z",
     "iopub.status.idle": "2023-07-23T17:10:09.797233Z",
     "shell.execute_reply": "2023-07-23T17:10:09.796141Z",
     "shell.execute_reply.started": "2023-07-23T17:10:08.505385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST_MEAN = 0.13\n",
    "MNIST_STD = 0.31\n",
    "\n",
    "def standardize_transform(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Standardizes a given tensor using the precomputed MNIST mean and standard deviation.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor to standardize.\n",
    "\n",
    "    Returns:\n",
    "        Standardized tensor.\n",
    "    \"\"\"\n",
    "    return (x - MNIST_MEAN) / MNIST_STD\n",
    "\n",
    "\n",
    "def standardize_inverse(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reverses the standardization on a tensor using the precomputed MNIST mean and standard deviation.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor to de-standardize.\n",
    "\n",
    "    Returns:\n",
    "        De-standardized tensor.\n",
    "    \"\"\"\n",
    "    return x * MNIST_STD + MNIST_MEAN\n",
    "\n",
    "\n",
    "class UnsupervisedMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    A class that wraps the MNIST dataset for unsupervised learning. It allows to work with either the Fashion MNIST\n",
    "    or the original MNIST dataset. This dataset will be used for the Autoencoder and VAE training (in the unsupervised manner).\n",
    "    \"\"\"\n",
    "    def __init__(self, train: bool = True, fashion: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object, downloads the dataset and prepares the transform.\n",
    "\n",
    "        Args:\n",
    "            train: Whether to load the training split (True) or the test split (False).\n",
    "            fashion: Whether to load the Fashion MNIST dataset (True) or the original MNIST dataset (False).\n",
    "        \"\"\"\n",
    "        cls = torchvision.datasets.FashionMNIST if fashion else torchvision.datasets.MNIST\n",
    "        self._mnist = cls(root='/tmp/mnist', train=train, download=True)\n",
    "        self._to_tensor = transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        raw_img, _ = self._mnist[i]\n",
    "        img = self._to_tensor(raw_img)\n",
    "        return standardize_transform(img)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._mnist)\n",
    "\n",
    "    def label(self, i: int) -> int:\n",
    "        \"\"\"\n",
    "        Returns the label of an image in the dataset at a given index.\n",
    "        This is only required for the future visualizations.\n",
    "\n",
    "        Args:\n",
    "            i: The index of the image for which to return the label.\n",
    "\n",
    "        Returns:\n",
    "            The label of the image at the given index.\n",
    "        \"\"\"\n",
    "        return self._mnist[i][1]\n",
    "    \n",
    "    \n",
    "mnist_example = UnsupervisedMNIST(train=True)\n",
    "mnist_example[0].shape, mnist_example.label(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the AutoEncoder neural network model\n",
    "\n",
    "In this section, an Autoencoder neural network is assembled. This is followed by the development of a Variational Autoencoder, which employs a similar structure. Although a latent space of two dimensions may not accurately represent the entirety of the MNIST dataset, it is utilized here for the simplicity of visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:10:09.799425Z",
     "iopub.status.busy": "2023-07-23T17:10:09.798867Z",
     "iopub.status.idle": "2023-07-23T17:10:09.896499Z",
     "shell.execute_reply": "2023-07-23T17:10:09.895540Z",
     "shell.execute_reply.started": "2023-07-23T17:10:09.799389Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Encoder transforms---\n",
      "Input shape: torch.Size([1, 28, 28])\n",
      "[1] name=Flatten, shape=torch.Size([784])\n",
      "[2] name=Linear, shape=torch.Size([392])\n",
      "[3] name=ReLU, shape=torch.Size([392])\n",
      "[4] name=Linear, shape=torch.Size([196])\n",
      "[5] name=ReLU, shape=torch.Size([196])\n",
      "[6] name=Linear, shape=torch.Size([2])\n",
      "---Decoder transforms---\n",
      "[0] Input shape: torch.Size([2])\n",
      "[1] name=Linear, shape=torch.Size([196])\n",
      "[2] name=ReLU, shape=torch.Size([196])\n",
      "[3] name=Linear, shape=torch.Size([392])\n",
      "[4] name=ReLU, shape=torch.Size([392])\n",
      "[5] name=Linear, shape=torch.Size([784])\n",
      "[6] name=Unflatten, shape=torch.Size([1, 28, 28])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of AutoEncoder (MLP).\n",
    "    MLP might be more efficient than CNN for very shallow networks (just like in this case).\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int = 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latent_dim: Latent dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Expects input shape 1x28x28\n",
    "        self._encoder = nn.Sequential(\n",
    "            nn.Flatten(start_dim=-3),  # size: 28*28 = 784\n",
    "            nn.Linear(784, 392),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(392, 196),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(196, latent_dim)\n",
    "        )\n",
    "        # encoder ~ posterior surrogate approximation\n",
    "\n",
    "        self._decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 196),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(196, 392),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(392, 784),\n",
    "            nn.Unflatten(-1, (1, 28, 28))\n",
    "        )\n",
    "        # decoder - likelihood\n",
    "\n",
    "    def show_encoder_transforms(self, x: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Prints list of encoder layers and output shapes. Useful for debugging and model validation.\n",
    "\n",
    "        Args:\n",
    "            x: Random input (with shape 1, 28, 28)\n",
    "        \"\"\"\n",
    "        print('---Encoder transforms---')\n",
    "        print(f'Input shape: {x.shape}')\n",
    "        for i, layer in enumerate(self._encoder):\n",
    "            layer_name = type(layer).__name__\n",
    "            x = layer(x)\n",
    "            print(f'[{i+1}] name={layer_name}, shape={x.shape}')\n",
    "\n",
    "    def show_decoder_transforms(self, x: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Prints list of encoder layers and output shapes. Useful for debugging and model validation.\n",
    "\n",
    "        Args:\n",
    "            x: Random input (with shape 1, 28, 28)\n",
    "        \"\"\"\n",
    "        print('---Decoder transforms---')\n",
    "        print(f'[0] Input shape: {x.shape}')\n",
    "        for i, layer in enumerate(self._decoder):\n",
    "            layer_name = type(layer).__name__\n",
    "            x = layer(x)\n",
    "            print(f'[{i+1}] name={layer_name}, shape={x.shape}')\n",
    "        print()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        z = self._encoder(x)\n",
    "        x_hat = self._decoder(z)\n",
    "        return x_hat, z\n",
    "\n",
    "\n",
    "ae = AutoEncoder()\n",
    "ae.show_encoder_transforms(torch.randn(1, 28, 28, dtype=torch.float32))\n",
    "ae.show_decoder_transforms(torch.randn(2, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:10:09.901460Z",
     "iopub.status.busy": "2023-07-23T17:10:09.900561Z",
     "iopub.status.idle": "2023-07-23T17:16:37.053359Z",
     "shell.execute_reply": "2023-07-23T17:16:37.052114Z",
     "shell.execute_reply.started": "2023-07-23T17:10:09.901424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a227b28524cc4f76b6ed5210a7b65f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'UnsupervisedMNIST' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'UnsupervisedMNIST' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 20143, 20144) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/Desktop/main-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1133\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1133\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py:113\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    112\u001B[0m timeout \u001B[38;5;241m=\u001B[39m deadline \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic()\n\u001B[0;32m--> 113\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Empty\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:262\u001B[0m, in \u001B[0;36m_ConnectionBase.poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_readable()\n\u001B[0;32m--> 262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_poll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:429\u001B[0m, in \u001B[0;36mConnection._poll\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[0;32m--> 429\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py:936\u001B[0m, in \u001B[0;36mwait\u001B[0;34m(object_list, timeout)\u001B[0m\n\u001B[1;32m    935\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 936\u001B[0m     ready \u001B[38;5;241m=\u001B[39m \u001B[43mselector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    937\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/selectors.py:416\u001B[0m, in \u001B[0;36m_PollLikeSelector.select\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 416\u001B[0m     fd_event_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/main-venv/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001B[0m, in \u001B[0;36m_set_SIGCHLD_handler.<locals>.handler\u001B[0;34m(signum, frame)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandler\u001B[39m(signum, frame):\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# Python can still get and update the process status successfully.\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[43m_error_if_any_worker_fails\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m previous_handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid 20143) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [4], line 85\u001B[0m\n\u001B[1;32m     81\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mdict\u001B[39m(history)\n\u001B[1;32m     84\u001B[0m autoencoder \u001B[38;5;241m=\u001B[39m AutoEncoder()\n\u001B[0;32m---> 85\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_autoencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mautoencoder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mUnsupervisedMNIST\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mUnsupervisedMNIST\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mACCELERATOR\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [4], line 42\u001B[0m, in \u001B[0;36mtrain_autoencoder\u001B[0;34m(model, train_dataset, val_dataset, epochs, scheduler_step_size, batch_size, num_workers, accelerator)\u001B[0m\n\u001B[1;32m     39\u001B[0m n_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     41\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 42\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m     43\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     45\u001B[0m     img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mto(accelerator)\n",
      "File \u001B[0;32m~/Desktop/main-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Desktop/main-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1329\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_data(data)\n\u001B[1;32m   1328\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1329\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1330\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable:\n\u001B[1;32m   1332\u001B[0m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/main-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1295\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1291\u001B[0m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[1;32m   1292\u001B[0m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1295\u001B[0m         success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1296\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m   1297\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/Desktop/main-venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1146\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1145\u001B[0m     pids_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mstr\u001B[39m(w\u001B[38;5;241m.\u001B[39mpid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[0;32m-> 1146\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) exited unexpectedly\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(pids_str)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue\u001B[38;5;241m.\u001B[39mEmpty):\n\u001B[1;32m   1148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid(s) 20143, 20144) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "def train_autoencoder(\n",
    "    model: AutoEncoder,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    epochs: int = 30,\n",
    "    scheduler_step_size: int = 15,\n",
    "    batch_size: int = 64,\n",
    "    num_workers = 2,\n",
    "    accelerator: str = 'cpu'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Trains the AutoEncoder model on the provided datasets for a specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model: An instance of AutoEncoder that will be trained.\n",
    "        train_dataset: The dataset object used for training.\n",
    "        val_dataset: The dataset object used for validation.\n",
    "        epochs: The number of epochs to train the model. Defaults to 20.\n",
    "        scheduler_step_size: Step size for learning rate scheduler. Defaults to 10.\n",
    "        batch_size: The size of the batches for training and validation. Defaults to 64.\n",
    "        num_workers: Number of workers for data loading. Defaults to 2.\n",
    "        accelerator: The device on which to train the model. Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the model's training and validation losses.\n",
    "    \"\"\"\n",
    "    loss_func = nn.MSELoss()\n",
    "    model.to(accelerator)\n",
    "    optimizer = torch.optim.Adam(lr=1e-3, params=model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = scheduler_step_size)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size, num_workers=num_workers, shuffle=True)\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        total_loss = 0.0\n",
    "        n_steps = 0\n",
    "        \n",
    "        model.train()\n",
    "        for img in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            img = img.to(accelerator)\n",
    "            reconstructed_img, _ = model(img)\n",
    "            \n",
    "            loss = loss_func(img, reconstructed_img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.detach().cpu().item()\n",
    "            n_steps += 1\n",
    "            \n",
    "        scheduler.step()\n",
    "            \n",
    "        train_loss = total_loss / n_steps\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        n_steps = 0\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for img in val_dataloader:\n",
    "                img = img.to(accelerator)\n",
    "                reconstructed_img, _ = model(img)\n",
    "\n",
    "                loss = loss_func(img, reconstructed_img)\n",
    "                \n",
    "                total_loss += loss.detach().cpu().item()\n",
    "                n_steps += 1\n",
    "                \n",
    "        val_loss = total_loss / n_steps\n",
    "        \n",
    "        print(f'[Epoch={epoch:02d}]: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}')\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['epoch'].append(epoch)\n",
    "        \n",
    "    return dict(history)\n",
    "        \n",
    "\n",
    "autoencoder = AutoEncoder()\n",
    "history = train_autoencoder(\n",
    "    model=autoencoder,\n",
    "    train_dataset=UnsupervisedMNIST(train=True),\n",
    "    val_dataset=UnsupervisedMNIST(train=False),\n",
    "    accelerator=ACCELERATOR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder demonstration\n",
    "\n",
    "In this section autoencoder reconstruction and latent space will be showcased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:16:37.055766Z",
     "iopub.status.busy": "2023-07-23T17:16:37.055154Z",
     "iopub.status.idle": "2023-07-23T17:16:37.550832Z",
     "shell.execute_reply": "2023-07-23T17:16:37.549825Z",
     "shell.execute_reply.started": "2023-07-23T17:16:37.055718Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_naive_autoencoder_error_analysis_visualization(\n",
    "    model: AutoEncoder,\n",
    "    val_dataset: Dataset,\n",
    "    accelerator: str = 'cpu'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays a naive visualization for error analysis of an autoencoder.\n",
    "\n",
    "    This function generates and displays a visual comparison of original and reconstructed\n",
    "    images using a trained autoencoder model. The error analysis is considered naive because\n",
    "    it only visually inspects a small percentage of the individual reconstruction errors.\n",
    "\n",
    "    Args:\n",
    "        model: The trained autoencoder model to use for image reconstruction.\n",
    "        val_dataset: The dataset to use for generating the visualization. Typically,\n",
    "            this should be a validation or test dataset.\n",
    "        accelerator: The device to perform operations. Default is 'cpu'.\n",
    "            Options are 'cpu' or 'cuda' if GPU is available.\n",
    "    \"\"\"\n",
    "    N_SAMPLES = 6\n",
    "    indices = list(range(len(val_dataset)))\n",
    "    sampled_indices = random.choices(indices, k=N_SAMPLES)\n",
    "    model.to(accelerator)\n",
    "    \n",
    "    _, axs = plt.subplots(figsize=(2 * N_SAMPLES + 1, 5), nrows=2, ncols=N_SAMPLES)\n",
    "    for i in range(N_SAMPLES):\n",
    "        index = sampled_indices[i]\n",
    "        img = val_dataset[index]\n",
    "        img = img.to(accelerator)\n",
    "        img_hat, _ = model.forward(img)\n",
    "        img, img_hat = [v.detach().cpu().numpy() for v in [img, img_hat]]\n",
    "        \n",
    "        axs[0][i].imshow(img[0])\n",
    "        axs[1][i].imshow(img_hat[0])\n",
    "        axs[0][i].axis('off')\n",
    "        axs[1][i].axis('off')\n",
    "           \n",
    "    plt.show()\n",
    "\n",
    "show_naive_autoencoder_error_analysis_visualization(\n",
    "    model=autoencoder,\n",
    "    val_dataset=UnsupervisedMNIST(train=False),\n",
    "    accelerator=ACCELERATOR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model appears to reconstruct the images reasonably well, though a certain level of blurriness is evident. Improvements in image sharpness might be achieved by expanding the latent dimension space (e.g. `latent_dim=8`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:16:37.553167Z",
     "iopub.status.busy": "2023-07-23T17:16:37.552416Z",
     "iopub.status.idle": "2023-07-23T17:16:45.927076Z",
     "shell.execute_reply": "2023-07-23T17:16:45.925893Z",
     "shell.execute_reply.started": "2023-07-23T17:16:37.553129Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_ae_latent_space_distribution_visualization(\n",
    "    model: AutoEncoder,\n",
    "    val_dataset: Dataset,\n",
    "    accelerator: str = 'cpu'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays the distribution of data in the Autoencoder' latent space.\n",
    "\n",
    "    Args:\n",
    "        model: The autoencoder model.\n",
    "        val_dataset: Dataset used for the visualization.\n",
    "        accelerator: The device for calculations ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    COLORS = ['red', 'blue', 'green', 'yellow', 'orange', 'cyan', 'gray', 'black', 'purple', 'pink']\n",
    "    model.eval()\n",
    "    model.to(accelerator)\n",
    "    \n",
    "    zs = defaultdict(list)\n",
    "    for i in tqdm(range(len(val_dataset))):\n",
    "        img = val_dataset[i].to(accelerator)\n",
    "        label = val_dataset.label(i)\n",
    "        _, z = autoencoder(img)\n",
    "        \n",
    "        zs[label].append(z.detach().cpu())\n",
    "        \n",
    "    zs = {k: torch.stack(v) for k, v in zs.items()}\n",
    "    zs = dict(sorted(zs.items()))\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, (label, z) in enumerate(zs.items()):\n",
    "        z = z.numpy()\n",
    "        plt.scatter(z[:, 0], z[:, 1], color=COLORS[i], label=str(label))\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.title('Latent Space')\n",
    "    plt.xlabel('Z1')\n",
    "    plt.ylabel('Z2')\n",
    "    plt.show()\n",
    "    \n",
    "show_ae_latent_space_distribution_visualization(\n",
    "    model=autoencoder,\n",
    "    val_dataset=UnsupervisedMNIST(train=False),\n",
    "    accelerator=ACCELERATOR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overlapping class clusters and extensive spread in the latent space suggest potential underfitting of the model. Expanding the latent dimension could potentially result in better separation of clusters. However, this is not necessarily true for VAEs, where the KL component of the ELBO loss constrains clusters to be closer to the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder\n",
    "\n",
    "In this segment, the implementation of the Variational Autoencoder (VAE) model is carried out. Notably, the VAE model retains all capabilities of the Autoencoder (AE) model while also offering additional features such as:\n",
    "\n",
    "- generating fresh images by sampling from the prior;\n",
    "- creating new images influenced by an input, by sampling from the posterior;\n",
    "- performing interpolation over the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:16:45.928764Z",
     "iopub.status.busy": "2023-07-23T17:16:45.928380Z",
     "iopub.status.idle": "2023-07-23T17:16:45.978174Z",
     "shell.execute_reply": "2023-07-23T17:16:45.977111Z",
     "shell.execute_reply.started": "2023-07-23T17:16:45.928716Z"
    }
   },
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of VAE (MLP).\n",
    "    MLP might be more efficient than CNN for very shallow networks (just like in this case).\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int = 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latent_dim: Latent dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._latent_dim = latent_dim\n",
    "\n",
    "        self._encoder = nn.Sequential(\n",
    "            nn.Flatten(start_dim=-3),  # size: 28*28 = 784\n",
    "            nn.Linear(784, 392),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(392, 196),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(196, 2 * latent_dim)\n",
    "        )\n",
    "    \n",
    "        self._decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 196),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(196, 392),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(392, 784),\n",
    "            nn.Unflatten(-1, (1, 28, 28))\n",
    "        )\n",
    "        \n",
    "    def show_encoder_transforms(self, x: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Prints list of encoder layers and output shapes. Useful for debugging and model validation.\n",
    "\n",
    "        Args:\n",
    "            x: Random input (with shape 1, 28, 28)\n",
    "        \"\"\"\n",
    "        print('---Encoder transforms---')\n",
    "        print(f'Input shape: {x.shape}')\n",
    "        for i, layer in enumerate(self._encoder):\n",
    "            layer_name = type(layer).__name__\n",
    "            x = layer(x)\n",
    "            print(f'[{i+1}] name={layer_name}, shape={x.shape}')\n",
    "\n",
    "    def show_decoder_transforms(self, x: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Prints list of encoder layers and output shapes. Useful for debugging and model validation.\n",
    "\n",
    "        Args:\n",
    "            x: Random input (with shape 1, 28, 28)\n",
    "        \"\"\"\n",
    "        print('---Decoder transforms---')\n",
    "        print(f'[0] Input shape: {x.shape}')\n",
    "        for i, layer in enumerate(self._decoder):\n",
    "            layer_name = type(layer).__name__\n",
    "            x = layer(x)\n",
    "            print(f'[{i+1}] name={layer_name}, shape={x.shape}')\n",
    "        print()\n",
    "            \n",
    "    def forward(self, x: torch.Tensor, sample: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        z_all = self._encoder(x)\n",
    "        z_mean, z_log_var = z_all[..., :self._latent_dim], z_all[..., self._latent_dim:]\n",
    "        \n",
    "        if sample:\n",
    "            z_std = torch.sqrt(torch.exp(z_log_var))\n",
    "            e = torch.randn_like(z_mean)\n",
    "            z = z_mean + z_std * e\n",
    "        else:\n",
    "            z = z_mean\n",
    "            \n",
    "        x_hat = self._decoder(z)\n",
    "        return {\n",
    "            'x': x_hat,\n",
    "            'z': z,\n",
    "            'z_mean': z_mean,\n",
    "            'z_log_var': z_log_var\n",
    "        }\n",
    "    \n",
    "    def sample_from_prior(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates a sample from the prior distribution and decodes it into data space.\n",
    "\n",
    "        Returns:\n",
    "            The generated data sample.\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        z = torch.randn(self._latent_dim).to(device)\n",
    "        return self._decoder(z)\n",
    "    \n",
    "    def sample_from_posterior(self, x_evidence: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates a sample from the posterior distribution conditioned on evidence and decodes it into data space.\n",
    "\n",
    "        Args:\n",
    "            x_evidence: The evidence to condition the posterior distribution on.\n",
    "\n",
    "        Returns:\n",
    "            The generated data sample.\n",
    "        \"\"\"\n",
    "        output = self.forward(x_evidence, sample=True)\n",
    "        return output['x']\n",
    "    \n",
    "vae = VariationalAutoEncoder()\n",
    "vae.show_encoder_transforms(torch.randn(1, 28, 28, dtype=torch.float32))\n",
    "vae.show_decoder_transforms(torch.randn(2, dtype=torch.float32))\n",
    "output = vae(torch.randn(1, 28, 28, dtype=torch.float32))\n",
    "{k: v.shape for k, v in output.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training VAE\n",
    "\n",
    "The training process for the VAE tends to be more time-consuming than for the AE due to its inherent stochastic nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:16:45.980411Z",
     "iopub.status.busy": "2023-07-23T17:16:45.979783Z",
     "iopub.status.idle": "2023-07-23T17:16:45.989128Z",
     "shell.execute_reply": "2023-07-23T17:16:45.988306Z",
     "shell.execute_reply.started": "2023-07-23T17:16:45.980375Z"
    }
   },
   "outputs": [],
   "source": [
    "class GaussianELBOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes the ELBO loss for a multivariate Gaussian distribution with a diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, noise: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            noise: Value of the covariance matrix, a constant.\n",
    "                Increasing this value enhances the regularization effect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._noise = noise\n",
    "        \n",
    "    def forward(self, x_hat: torch.Tensor, x_target: torch.Tensor, z_mean: torch.Tensor, z_log_var: torch.Tensor) -> torch.Tensor:\n",
    "        likelihood_loss = torch.mean(1 / (2 * self._noise) * torch.sum(torch.square(x_hat - x_target), dim=-1))\n",
    "        z_var = torch.exp(z_log_var)\n",
    "        kl_loss = torch.mean(- 1/2 * torch.sum(1 + z_log_var / 2 - z_var - torch.square(z_mean), dim=-1))\n",
    "        return likelihood_loss + kl_loss, likelihood_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:16:45.991285Z",
     "iopub.status.busy": "2023-07-23T17:16:45.990661Z",
     "iopub.status.idle": "2023-07-23T17:25:41.075182Z",
     "shell.execute_reply": "2023-07-23T17:25:41.073969Z",
     "shell.execute_reply.started": "2023-07-23T17:16:45.991249Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_variational_autoencoder(\n",
    "    model: VariationalAutoEncoder,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    epochs: int = 40,\n",
    "    scheduler_step_size: int = 20,\n",
    "    batch_size: int = 64,\n",
    "    num_workers = 2,\n",
    "    accelerator: str = 'cpu'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Trains the Variational AutoEncoder (VAE) model and returns the training history.\n",
    "\n",
    "    Args:\n",
    "        model: The VariationalAutoEncoder model to be trained.\n",
    "        train_dataset: The dataset used for training.\n",
    "        val_dataset: The dataset used for validation during training.\n",
    "        epochs: The number of epochs to train the model. Default is 20.\n",
    "        scheduler_step_size: The number of epochs after which the scheduler decreases the learning rate. Default is 10.\n",
    "        batch_size: The size of the batches used for training and validation. Default is 64.\n",
    "        num_workers: The number of worker threads used with the DataLoader. Default is 2.\n",
    "        accelerator: The device to perform operations. Default is 'cpu'. Options are 'cpu' or 'cuda' if GPU is available.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the training history, including training and validation loss per epoch.\n",
    "    \"\"\"\n",
    "    loss_func = GaussianELBOLoss(noise=0.1)\n",
    "    model.to(accelerator)\n",
    "    optimizer = torch.optim.Adam(lr=1e-3, params=model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = scheduler_step_size)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size, num_workers=num_workers, shuffle=True)\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        total_loss = 0.0\n",
    "        total_likelihood_loss = 0.0\n",
    "        total_kl_loss = 0.0\n",
    "        n_steps = 0\n",
    "        \n",
    "        model.train()\n",
    "        for img in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            img = img.to(accelerator)\n",
    "            output = model(img)\n",
    "            \n",
    "            loss, likelihood_loss, kl_loss = loss_func(output['x'], img, output['z_mean'], output['z_log_var'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.detach().cpu().item()\n",
    "            total_likelihood_loss += likelihood_loss.detach().cpu().item()\n",
    "            total_kl_loss += kl_loss.detach().cpu().item()\n",
    "            n_steps += 1\n",
    "            \n",
    "        scheduler.step()\n",
    "            \n",
    "        train_loss = total_loss / n_steps\n",
    "        train_likelihood_loss = total_likelihood_loss / n_steps\n",
    "        train_kl_loss = total_kl_loss / n_steps\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_likelihood_loss = 0.0\n",
    "        total_kl_loss = 0.0\n",
    "        n_steps = 0\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for img in val_dataloader:\n",
    "                img = img.to(accelerator)\n",
    "                output = model(img)\n",
    "\n",
    "                loss, likelihood_loss, kl_loss = loss_func(output['x'], img, output['z_mean'], output['z_log_var'])\n",
    "                \n",
    "                total_loss += loss.detach().cpu().item()\n",
    "                total_likelihood_loss += likelihood_loss.detach().cpu().item()\n",
    "                total_kl_loss += kl_loss.detach().cpu().item()\n",
    "                n_steps += 1\n",
    "                \n",
    "        val_loss = total_loss / n_steps\n",
    "        c = total_likelihood_loss / n_steps\n",
    "        val_kl_loss = total_kl_loss / n_steps\n",
    "        \n",
    "        print(f'[Epoch={epoch:02d}]: train_loss={train_loss:.2f}, val_loss={val_loss:.2f}')\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_likelihood_loss'].append(train_likelihood_loss)\n",
    "        history['train_kl_loss'].append(train_kl_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_likelihood_loss'].append(val_kl_loss)\n",
    "        history['val_kl_loss'].append(val_kl_loss)\n",
    "        history['epoch'].append(epoch)\n",
    "        \n",
    "    return dict(history)\n",
    "        \n",
    "\n",
    "vae = VariationalAutoEncoder()\n",
    "history = train_variational_autoencoder(\n",
    "    model=vae,\n",
    "    train_dataset=UnsupervisedMNIST(train=True),\n",
    "    val_dataset=UnsupervisedMNIST(train=False),\n",
    "    accelerator=ACCELERATOR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:25:41.080595Z",
     "iopub.status.busy": "2023-07-23T17:25:41.079520Z",
     "iopub.status.idle": "2023-07-23T17:25:41.803877Z",
     "shell.execute_reply": "2023-07-23T17:25:41.802927Z",
     "shell.execute_reply.started": "2023-07-23T17:25:41.080556Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_vae_training_history(history: dict) -> None:\n",
    "    \"\"\"\n",
    "    Displays the training history of the Variational Autoencoder (VAE) model.\n",
    "\n",
    "    Args:\n",
    "        history: A dictionary containing the training history.\n",
    "    \"\"\"\n",
    "\n",
    "    _, axs = plt.subplots(figsize=(14, 4), ncols=3)\n",
    "    \n",
    "    axs[0].plot(history['train_loss'], color='orange', label='train')\n",
    "    axs[0].plot(history['val_loss'], color='blue', label='val')\n",
    "    axs[0].set_xlabel('epoch')\n",
    "    axs[0].set_ylabel('loss')\n",
    "    axs[0].set_title('Loss History')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(history['train_likelihood_loss'], color='orange', label='train')\n",
    "    axs[1].plot(history['val_likelihood_loss'], color='blue', label='val')\n",
    "    axs[1].set_xlabel('epoch')\n",
    "    axs[1].set_ylabel('loss')\n",
    "    axs[1].set_title('Likelihood Loss History')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    axs[2].plot(history['train_kl_loss'], color='orange', label='train')\n",
    "    axs[2].plot(history['val_kl_loss'], color='blue', label='val')\n",
    "    axs[2].set_xlabel('epoch')\n",
    "    axs[2].set_ylabel('loss')\n",
    "    axs[2].set_title('KL Loss History')\n",
    "    axs[2].legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "show_vae_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE demonstration\n",
    "\n",
    "##### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:25:41.807159Z",
     "iopub.status.busy": "2023-07-23T17:25:41.805405Z",
     "iopub.status.idle": "2023-07-23T17:25:42.487139Z",
     "shell.execute_reply": "2023-07-23T17:25:42.485639Z",
     "shell.execute_reply.started": "2023-07-23T17:25:41.807122Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_naive_variational_autoencoder_error_analysis_visualization(\n",
    "    model: VariationalAutoEncoder,\n",
    "    val_dataset: Dataset,\n",
    "    accelerator: str = 'cpu'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays a naive visualization for error analysis of a VAE.\n",
    "\n",
    "    Args:\n",
    "        model: The trained VAE model to use for image reconstruction.\n",
    "        val_dataset: The dataset to use for generating the visualization. Typically,\n",
    "            this should be a validation or test dataset.\n",
    "        accelerator: The device to perform operations. Default is 'cpu'.\n",
    "            Options are 'cpu' or 'cuda' if GPU is available.\n",
    "    \"\"\"\n",
    "    N_SAMPLES = 6\n",
    "    indices = list(range(len(val_dataset)))\n",
    "    sampled_indices = random.choices(indices, k=N_SAMPLES)\n",
    "    model.to(accelerator)\n",
    "    \n",
    "    _, axs = plt.subplots(figsize=(2 * N_SAMPLES + 1, 5), nrows=2, ncols=N_SAMPLES)\n",
    "    for i in range(N_SAMPLES):\n",
    "        index = sampled_indices[i]\n",
    "        img = val_dataset[index]\n",
    "        img = img.to(accelerator)\n",
    "        output = model.forward(img)\n",
    "        img_hat = output['x']\n",
    "        img, img_hat = [v.detach().cpu().numpy() for v in [img, img_hat]]\n",
    "        \n",
    "        axs[0][i].imshow(img[0])\n",
    "        axs[1][i].imshow(img_hat[0])\n",
    "        axs[0][i].axis('off')\n",
    "        axs[1][i].axis('off')\n",
    "           \n",
    "    plt.show()\n",
    "\n",
    "show_naive_variational_autoencoder_error_analysis_visualization(\n",
    "    model=vae,\n",
    "    val_dataset=UnsupervisedMNIST(train=False),\n",
    "    accelerator=ACCELERATOR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:25:42.489107Z",
     "iopub.status.busy": "2023-07-23T17:25:42.488724Z",
     "iopub.status.idle": "2023-07-23T17:25:51.360209Z",
     "shell.execute_reply": "2023-07-23T17:25:51.359306Z",
     "shell.execute_reply.started": "2023-07-23T17:25:42.489071Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_vae_map_latent_space_distribution_visualization(\n",
    "    model: VariationalAutoEncoder,\n",
    "    val_dataset: Dataset,\n",
    "    accelerator: str = 'cpu'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays the distribution of data in the VAE latent space.\n",
    "\n",
    "    Args:\n",
    "        model: The VAE model.\n",
    "        val_dataset: Dataset used for the visualization.\n",
    "        accelerator: The device for calculations ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    COLORS = ['red', 'blue', 'green', 'yellow', 'orange', 'cyan', 'gray', 'black', 'purple', 'pink']\n",
    "    model.eval()\n",
    "    model.to(accelerator)\n",
    "    \n",
    "    zs = defaultdict(list)\n",
    "    for i in tqdm(range(len(val_dataset))):\n",
    "        img = val_dataset[i].to(accelerator)\n",
    "        label = val_dataset.label(i)\n",
    "        output = model.forward(img, sample=False)\n",
    "        z = output['z_mean']\n",
    "        \n",
    "        zs[label].append(z.detach().cpu())\n",
    "        \n",
    "    zs = {k: torch.stack(v) for k, v in zs.items()}\n",
    "    zs = dict(sorted(zs.items()))\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, (label, z) in enumerate(zs.items()):\n",
    "        z = z.numpy()\n",
    "        plt.scatter(z[:, 0], z[:, 1], color=COLORS[i], label=str(label))\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.title('Latent Space')\n",
    "    plt.xlabel('Z1')\n",
    "    plt.ylabel('Z2')\n",
    "    plt.show()\n",
    "    \n",
    "show_vae_map_latent_space_distribution_visualization(\n",
    "    model=vae,\n",
    "    val_dataset=UnsupervisedMNIST(train=False),\n",
    "    accelerator=ACCELERATOR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating new samples (sampling from the prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:25:51.362327Z",
     "iopub.status.busy": "2023-07-23T17:25:51.361687Z",
     "iopub.status.idle": "2023-07-23T17:25:52.230219Z",
     "shell.execute_reply": "2023-07-23T17:25:52.229232Z",
     "shell.execute_reply.started": "2023-07-23T17:25:51.362290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample from prior\n",
    "def visualize_vae_sampling_from_prior(\n",
    "    model: VariationalAutoEncoder,\n",
    "    accelerator: str = 'cpu'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates few samples from the VAE prior.\n",
    "\n",
    "    Args:\n",
    "        model: The VAE model.\n",
    "        accelerator: The device for calculations ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    model.to(accelerator)\n",
    "    model.eval()\n",
    "    \n",
    "    _, axs = plt.subplots(figsize=(14, 10), nrows=5, ncols=5)\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            ax = axs[i][j]\n",
    "            img = vae.sample_from_prior()\n",
    "            ax.imshow(img.detach().cpu().numpy()[0])\n",
    "            ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "visualize_vae_sampling_from_prior(\n",
    "    model=vae,\n",
    "    accelerator=ACCELERATOR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating new samples similar to the given one (sampling from the posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:25:52.232555Z",
     "iopub.status.busy": "2023-07-23T17:25:52.231762Z",
     "iopub.status.idle": "2023-07-23T17:25:53.299039Z",
     "shell.execute_reply": "2023-07-23T17:25:53.298074Z",
     "shell.execute_reply.started": "2023-07-23T17:25:52.232516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample from posterior\n",
    "def visualize_vae_sampling_from_posterior(\n",
    "    model: VariationalAutoEncoder,\n",
    "    evidence: torch.Tensor,\n",
    "    accelerator: str = 'cpu'\n",
    ") -> None:\n",
    "    model.to(accelerator)\n",
    "    model.eval()\n",
    "    evidence = evidence.to(accelerator)\n",
    "    \n",
    "    _, axs = plt.subplots(figsize=(14, 10), nrows=5, ncols=5)\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            ax = axs[i][j]\n",
    "            img = vae.sample_from_posterior(evidence)\n",
    "            ax.imshow(img.detach().cpu().numpy()[0])\n",
    "            ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "visualize_vae_sampling_from_posterior(\n",
    "    model=vae,\n",
    "    evidence=UnsupervisedMNIST(train=False)[7865],\n",
    "    accelerator=ACCELERATOR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpolation over the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:25:53.301442Z",
     "iopub.status.busy": "2023-07-23T17:25:53.300354Z",
     "iopub.status.idle": "2023-07-23T17:25:56.632099Z",
     "shell.execute_reply": "2023-07-23T17:25:56.631236Z",
     "shell.execute_reply.started": "2023-07-23T17:25:53.301390Z"
    }
   },
   "outputs": [],
   "source": [
    "# Interpolation\n",
    "def vizualize_vae_interpolation(\n",
    "    model: VariationalAutoEncoder,\n",
    "    accelerator: str = 'cpu'\n",
    ") -> None:\n",
    "    model.to(accelerator)\n",
    "    model.eval()\n",
    "    \n",
    "    _, axs = plt.subplots(figsize=(16, 10), nrows=10, ncols=10)\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            ax = axs[i][j]\n",
    "            z = torch.tensor([-1 + 2 * (i / 10), -1 + 2 * (j / 10)], dtype=torch.float32)\n",
    "            z = z.to(accelerator)\n",
    "            img = vae._decoder(z)\n",
    "            ax.imshow(img.detach().cpu().numpy()[0])\n",
    "            ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "vizualize_vae_interpolation(\n",
    "    model=vae,\n",
    "    accelerator=ACCELERATOR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training VAE on the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:25:56.634476Z",
     "iopub.status.busy": "2023-07-23T17:25:56.633543Z",
     "iopub.status.idle": "2023-07-23T17:34:54.034225Z",
     "shell.execute_reply": "2023-07-23T17:34:54.032926Z",
     "shell.execute_reply.started": "2023-07-23T17:25:56.634441Z"
    }
   },
   "outputs": [],
   "source": [
    "fashion_vae = VariationalAutoEncoder()\n",
    "history = train_variational_autoencoder(\n",
    "    model=vae,\n",
    "    train_dataset=UnsupervisedMNIST(train=True, fashion=True),\n",
    "    val_dataset=UnsupervisedMNIST(train=False, fashion=True),\n",
    "    accelerator=ACCELERATOR\n",
    ")\n",
    "show_vae_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T17:34:54.036579Z",
     "iopub.status.busy": "2023-07-23T17:34:54.035934Z",
     "iopub.status.idle": "2023-07-23T17:34:57.735223Z",
     "shell.execute_reply": "2023-07-23T17:34:57.734381Z",
     "shell.execute_reply.started": "2023-07-23T17:34:54.036536Z"
    }
   },
   "outputs": [],
   "source": [
    "vizualize_vae_interpolation(\n",
    "    model=fashion_vae,\n",
    "    accelerator=ACCELERATOR\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
